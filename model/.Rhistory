p + geom_density() +
facet_wrap(~variable, scales="free")
#NOTES:
# - be careful of using variables that get created AFTER a loan is issued (prinicpal/interest related)
# - any ID variables that are numeric will be plotted as well. be sure to ignore those as well.
#################################
# only evaluate w/ vintages that have come to term
df.term <- subset(df, year_issued < 2012)
df.term$home_ownership <- factor(df.term$home_ownership)
df.term$is_rent <- df.term$home_ownership=="RENT"
idx <- runif(nrow(df.term)) > 0.75
train <- df.term[idx==FALSE,]
test <- df.term[idx==TRUE,]
my.glm <- glm(is_bad ~ last_fico_range_low + last_fico_range_high +  is_rent, data=train
, na.action=na.omit, family=binomial())
my.glm$data <- NULL
summary(my.glm)
library(stringr)
library(plyr)
library(lubridate)
library(randomForest)
library(reshape2)
library(ggplot2)
file <- "./LoanStats3a.csv"
df <- read.csv(file, h=T, stringsAsFactors=F, skip=1)
df.head <- head(df, 100)
#take a peak...
head(df)
#annoying column; just get rid of it
df[,'desc'] <- NULL
summary(df)
#almost all NA, so just get rid of it
df[,'mths_since_last_record'] <- NULL
#get rid of fields that are mainly NA
poor_coverage <- sapply(df, function(x) {
coverage <- 1 - sum(is.na(x)) / length(x)
coverage < 0.8
})
df <- df[,poor_coverage==FALSE]
##################
bad_indicators <- c("Late (16-30 days)", "Late (31-120 days)", "Default", "Charged Off")
df$is_bad <- ifelse(df$loan_status %in% bad_indicators, 1,
ifelse(df$loan_status=="", NA,
0))
table(df$loan_status)
table(df$is_bad)
head(df)
df$issue_d <- as.Date(df$issue_d)
df$year_issued <- year(df$issue_d)
df$month_issued <- month(df$issue_d)
df$earliest_cr_line <- as.Date(df$earliest_cr_line)
df$revol_util <- str_replace_all(df$revol_util, "[%]", "")
df$revol_util <- as.numeric(df$revol_util)
outcomes <- ddply(df, .(year_issued, month_issued), function(x) {
c("percent_bad"=sum(x$is_bad) / nrow(x),
"n_loans"=nrow(x))
})
plot(outcomes$percent_bad, main="Bad Rate")
outcomes
###############################
#figure out which columns are numeirc (and hence we can look at the distribution)
numeric_cols <- sapply(df, is.numeric)
#turn the data into long format (key->value esque)
df.lng <- melt(df[,numeric_cols], id="is_bad")
head(df.lng)
#plot the distribution for bads and goods for each variable
p <- ggplot(aes(x=value, group=is_bad, colour=factor(is_bad)), data=df.lng)
#quick and dirty way to figure out if you have any good variables
p + geom_density() +
facet_wrap(~variable, scales="free")
#NOTES:
# - be careful of using variables that get created AFTER a loan is issued (prinicpal/interest related)
# - any ID variables that are numeric will be plotted as well. be sure to ignore those as well.
#################################
# only evaluate w/ vintages that have come to term
df.term <- subset(df, year_issued < 2012)
df.term$home_ownership <- factor(df.term$home_ownership)
df.term$is_rent <- df.term$home_ownership=="RENT"
idx <- runif(nrow(df.term)) > 0.75
train <- df.term[idx==FALSE,]
test <- df.term[idx==TRUE,]
my.glm <- glm(is_bad ~ last_fico_range_low + last_fico_range_high +  is_rent, data=train
, na.action=na.omit, family=binomial())
my.glm$data <- NULL
summary(my.glm)
translateToScore <- function(df) {
# takes log odds and converts them to a traditional credit score between 300 and 850
baseline <- 600
baseline + predict(my.glm, newdata=df) * (40 / log(2))
}
library(stringr)
library(plyr)
library(lubridate)
library(randomForest)
library(reshape2)
library(ggplot2)
file <- "./LoanStats3a.csv"
df <- read.csv(file, h=T, stringsAsFactors=F, skip=1)
df.head <- head(df, 100)
#take a peak...
head(df)
#annoying column; just get rid of it
df[,'desc'] <- NULL
summary(df)
#almost all NA, so just get rid of it
df[,'mths_since_last_record'] <- NULL
#get rid of fields that are mainly NA
poor_coverage <- sapply(df, function(x) {
coverage <- 1 - sum(is.na(x)) / length(x)
coverage < 0.8
})
df <- df[,poor_coverage==FALSE]
##################
bad_indicators <- c("Late (16-30 days)", "Late (31-120 days)", "Default", "Charged Off")
df$is_bad <- ifelse(df$loan_status %in% bad_indicators, 1,
ifelse(df$loan_status=="", NA,
0))
table(df$loan_status)
table(df$is_bad)
head(df)
df$issue_d <- as.Date(df$issue_d)
df$year_issued <- year(df$issue_d)
df$month_issued <- month(df$issue_d)
df$earliest_cr_line <- as.Date(df$earliest_cr_line)
df$revol_util <- str_replace_all(df$revol_util, "[%]", "")
df$revol_util <- as.numeric(df$revol_util)
outcomes <- ddply(df, .(year_issued, month_issued), function(x) {
c("percent_bad"=sum(x$is_bad) / nrow(x),
"n_loans"=nrow(x))
})
plot(outcomes$percent_bad, main="Bad Rate")
outcomes
###############################
#figure out which columns are numeirc (and hence we can look at the distribution)
numeric_cols <- sapply(df, is.numeric)
#turn the data into long format (key->value esque)
df.lng <- melt(df[,numeric_cols], id="is_bad")
head(df.lng)
#plot the distribution for bads and goods for each variable
p <- ggplot(aes(x=value, group=is_bad, colour=factor(is_bad)), data=df.lng)
#quick and dirty way to figure out if you have any good variables
p + geom_density() +
facet_wrap(~variable, scales="free")
library(stringr)
library(plyr)
library(lubridate)
library(randomForest)
library(reshape2)
library(ggplot2)
install.packages("lubridate")
library(stringr)
library(plyr)
library(lubridate)
library(randomForest)
library(reshape2)
library(ggplot2)
file <- "./LoanStats3a.csv"
df <- read.csv(file, h=T, stringsAsFactors=F, skip=1)
df.head <- head(df, 100)
#take a peak...
head(df)
#annoying column; just get rid of it
df[,'desc'] <- NULL
summary(df)
#almost all NA, so just get rid of it
df[,'mths_since_last_record'] <- NULL
#get rid of fields that are mainly NA
poor_coverage <- sapply(df, function(x) {
coverage <- 1 - sum(is.na(x)) / length(x)
coverage < 0.8
})
df <- df[,poor_coverage==FALSE]
##################
bad_indicators <- c("Late (16-30 days)", "Late (31-120 days)", "Default", "Charged Off")
df$is_bad <- ifelse(df$loan_status %in% bad_indicators, 1,
ifelse(df$loan_status=="", NA,
0))
table(df$loan_status)
table(df$is_bad)
head(df)
df$issue_d <- as.Date(df$issue_d)
df$year_issued <- year(df$issue_d)
df$month_issued <- month(df$issue_d)
df$earliest_cr_line <- as.Date(df$earliest_cr_line)
df$revol_util <- str_replace_all(df$revol_util, "[%]", "")
df$revol_util <- as.numeric(df$revol_util)
outcomes <- ddply(df, .(year_issued, month_issued), function(x) {
c("percent_bad"=sum(x$is_bad) / nrow(x),
"n_loans"=nrow(x))
})
plot(outcomes$percent_bad, main="Bad Rate")
outcomes
###############################
#figure out which columns are numeirc (and hence we can look at the distribution)
numeric_cols <- sapply(df, is.numeric)
#turn the data into long format (key->value esque)
df.lng <- melt(df[,numeric_cols], id="is_bad")
head(df.lng)
#plot the distribution for bads and goods for each variable
p <- ggplot(aes(x=value, group=is_bad, colour=factor(is_bad)), data=df.lng)
#quick and dirty way to figure out if you have any good variables
p + geom_density() +
facet_wrap(~variable, scales="free")
#NOTES:
# - be careful of using variables that get created AFTER a loan is issued (prinicpal/interest related)
# - any ID variables that are numeric will be plotted as well. be sure to ignore those as well.
#################################
# only evaluate w/ vintages that have come to term
df.term <- subset(df, year_issued < 2012)
df.term$home_ownership <- factor(df.term$home_ownership)
df.term$is_rent <- df.term$home_ownership=="RENT"
idx <- runif(nrow(df.term)) > 0.75
train <- df.term[idx==FALSE,]
test <- df.term[idx==TRUE,]
my.glm <- glm(is_bad ~ last_fico_range_low + last_fico_range_high +  is_rent, data=train
, na.action=na.omit, family=binomial())
my.glm$data <- NULL
summary(my.glm)
translateToScore <- function(df) {
# takes log odds and converts them to a traditional credit score between 300 and 850
baseline <- 600
baseline + predict(my.glm, newdata=df) * (40 / log(2))
}
library(lubridate)
library(stringr)
library(plyr)
library(lubridate)
library(randomForest)
library(reshape2)
install.pacakges("randomForest")
install("randomForest")
install.packages("randomForest")
library(stringr)
library(plyr)
library(lubridate)
library(randomForest)
library(reshape2)
library(ggplot2)
file <- "./LoanStats3a.csv"
df <- read.csv(file, h=T, stringsAsFactors=F, skip=1)
setwd("~/repos/yhat/demos/heroku-demos/demo-lending-club")
file <- "./LoanStats3a.csv"
df <- read.csv(file, h=T, stringsAsFactors=F, skip=1)
df.head <- head(df, 100)
#take a peak...
head(df)
#annoying column; just get rid of it
df[,'desc'] <- NULL
summary(df)
file <- "./LoanStats3a.csv"
df <- read.csv(file, h=T, stringsAsFactors=F, skip=1)
setwd("~/repos/yhat/demos/heroku-demos/demo-lending-club/model/")
file <- "./LoanStats3a.csv"
df <- read.csv(file, h=T, stringsAsFactors=F, skip=1)
df.head <- head(df, 100)
#take a peak...
head(df)
#annoying column; just get rid of it
df[,'desc'] <- NULL
summary(df)
#almost all NA, so just get rid of it
df[,'mths_since_last_record'] <- NULL
#get rid of fields that are mainly NA
poor_coverage <- sapply(df, function(x) {
coverage <- 1 - sum(is.na(x)) / length(x)
coverage < 0.8
})
df <- df[,poor_coverage==FALSE]
##################
bad_indicators <- c("Late (16-30 days)", "Late (31-120 days)", "Default", "Charged Off")
df$is_bad <- ifelse(df$loan_status %in% bad_indicators, 1,
ifelse(df$loan_status=="", NA,
0))
table(df$loan_status)
table(df$is_bad)
head(df)
df$issue_d <- as.Date(df$issue_d)
df$year_issued <- year(df$issue_d)
df$month_issued <- month(df$issue_d)
df$earliest_cr_line <- as.Date(df$earliest_cr_line)
df$revol_util <- str_replace_all(df$revol_util, "[%]", "")
df$revol_util <- as.numeric(df$revol_util)
outcomes <- ddply(df, .(year_issued, month_issued), function(x) {
c("percent_bad"=sum(x$is_bad) / nrow(x),
"n_loans"=nrow(x))
})
plot(outcomes$percent_bad, main="Bad Rate")
outcomes
###############################
#figure out which columns are numeirc (and hence we can look at the distribution)
numeric_cols <- sapply(df, is.numeric)
#turn the data into long format (key->value esque)
df.lng <- melt(df[,numeric_cols], id="is_bad")
head(df.lng)
#plot the distribution for bads and goods for each variable
p <- ggplot(aes(x=value, group=is_bad, colour=factor(is_bad)), data=df.lng)
#quick and dirty way to figure out if you have any good variables
p + geom_density() +
facet_wrap(~variable, scales="free")
#NOTES:
# - be careful of using variables that get created AFTER a loan is issued (prinicpal/interest related)
# - any ID variables that are numeric will be plotted as well. be sure to ignore those as well.
#################################
# only evaluate w/ vintages that have come to term
df.term <- subset(df, year_issued < 2012)
df.term$home_ownership <- factor(df.term$home_ownership)
df.term$is_rent <- df.term$home_ownership=="RENT"
idx <- runif(nrow(df.term)) > 0.75
train <- df.term[idx==FALSE,]
test <- df.term[idx==TRUE,]
my.glm <- glm(is_bad ~ last_fico_range_low + last_fico_range_high +  is_rent, data=train
, na.action=na.omit, family=binomial())
my.glm$data <- NULL
translateToScore <- function(df) {
# takes log odds and converts them to a traditional credit score between 300 and 850
baseline <- 600
}
summary(my.glm)
library(yhatr)
baseline + predict(my.glm, newdata=df) * (40 / log(2))
######################################
# define external dependencies
model.require <- function() {
library(plyr)
}
# execute your code/model
model.predict <- function(df) {
df$is_rent <- df$home_ownership=="RENT"
prediction <- predict(my.glm, newdata=df, type="response")
output <- data.frame(prob_default=prediction)
output$score <- translateToScore(df)
output$decline_code <- ifelse(output$prob_default > 0.3, "Credit score too low", "")
output
}
model.predict(df[1,])
translateToScore(df)
translateToScore
translateToScore <- function(df) {
# takes log odds and converts them to a traditional credit score between 300 and 850
baseline <- 600
baseline + predict(my.glm, newdata=df) * (40 / log(2))
}
translateToScore(df)
df
translateToScore(train)
hist(translateToScore(train))
h = hist(translateToScore(train))
?hist
h$breaks
h$counts
h[,c("breaks", "counts")]
toJSON(h$counts)
toJSON(h$breaks)
translateToScore <- function(df) {
# takes log odds and converts them to a traditional credit score between 300 and 850
baseline <- 600
baseline + predict(my.glm, newdata=df) * (40 / log(2))
}
######################################
library(yhatr)
# define external dependencies
model.require <- function() {
library(plyr)
}
# execute your code/model
model.predict <- function(df) {
df$is_rent <- df$home_ownership=="RENT"
prediction <- predict(my.glm, newdata=df, type="response")
output <- data.frame(prob_default=prediction)
output$score <- translateToScore(df)
output$decline_code <- ifelse(output$prob_default > 0.3, "Credit score too low", "")
output
}
model.predict(df[1,])
yhat.config <- c(
username="demo-master",
apikey="4a662eb13647cfb9ed4ca36c5e95c7b3",
env="http://sandbox.yhathq.com/"
)
yhat.deploy("LendingClub")
yhat.config <- c(
username="demo-master",
apikey="4a662eb13647cfb9ed4ca36c5e95c7b3",
env="https://sandbox.yhathq.com/"
)
yhat.deploy("LendingClub")
predict(my.glm, newdata=df)
translateToScore <- function(df) {
# takes log odds and converts them to a traditional credit score between 300 and 850
baseline <- 600
baseline - predict(my.glm, newdata=df) * (40 / log(2))
}
######################################
library(yhatr)
# define external dependencies
model.require <- function() {
library(plyr)
}
# execute your code/model
model.predict <- function(df) {
df$is_rent <- df$home_ownership=="RENT"
prediction <- predict(my.glm, newdata=df, type="response")
output <- data.frame(probability=prediction)
output$score <- translateToScore(df)
output$decline_code <- ifelse(output$probability > 0.3, "Credit score too low", "")
output
}
model.predict(df[1,])
yhat.config <- c(
username="demo-master",
apikey="4a662eb13647cfb9ed4ca36c5e95c7b3",
env="https://sandbox.yhathq.com/"
)
yhat.deploy("LendingClub")
toJSON(h$counts)
h = hist(translateToScore(train))
toJSON(h$counts)
toJSON(h$breaks)
translateToScore(train)
translateToScore <- function(df) {
# takes log odds and converts them to a traditional credit score between 300 and 850
baseline <- 600
baseline + predict(my.glm, newdata=df) * (40 / log(2))
}
translateToScore(train)
hist(translateToScore(train))
predict(my.glm, newdata=df)
predict(my.glm, newdata=train)
hist(predict(my.glm, newdata=train))
hist(predict(my.glm, newdata=train) *  (40 / log(2)))
hist(predict(my.glm) *  (40 / log(2)), predict(my.glm, type="response"))
plot(predict(my.glm) *  (40 / log(2)), predict(my.glm, type="response"))
plot(-predict(my.glm) *  (40 / log(2)), predict(my.glm, type="response"))
plot(baseline + -predict(my.glm) *  (40 / log(2)), predict(my.glm, type="response"))
plot(baseline -predict(my.glm) *  (40 / log(2)), predict(my.glm, type="response"))
baseline <- 600
plot(baseline -predict(my.glm) *  (40 / log(2)), predict(my.glm, type="response"))
my.glm <- glm(I(is_bad==FALSE) ~ last_fico_range_low + last_fico_range_high +  is_rent, data=train
, na.action=na.omit, family=binomial())
translateToScore <- function(df) {
# takes log odds and converts them to a traditional credit score between 300 and 850
baseline <- 600
baseline + predict(my.glm, newdata=df) * (40 / log(2))
}
######################################
plot(baseline + predict(my.glm) *  (40 / log(2)), predict(my.glm, type="response"))
translateToScore(predict(train))
hist(translateToScore(train))
readLines
readLines(file("~/Desktop/nationalfunding.json"))
readLines
fileName <- "~/Desktop/nationalfunding.json"
readChar(fileName, file.info(fileName)$size)
library(jsonlite)
library(rjson)
rawJSON <- readChar(fileName, file.info(fileName)$size)
jsonlite::fromJSON(rawJSON)
rjson::fromJSON(rawJSON)
str(rjson::fromJSON(rawJSON))
rjson::fromJSON(rawJSON)
crs.json <- readChar(fileName, file.info(fileName)$size)
wccd.df <- jsonlite::fromJSON(crs.json)$SFObjects$wc_credit_decisions
cr.df <- jsonlite::fromJSON(crs.json)$SFObjects$credit_reviews
opp.df <- jsonlite::fromJSON(crs.json)$SFObjects$opportunities
crbs.df <- jsonlite::fromJSON(crs.json)$SFObjects$bank_summaries
crml.df <- jsonlite::fromJSON(crs.json)$SFObjects$ledgers
crmr.df <- jsonlite::fromJSON(crs.json)$SFObjects$monthly_reviews
wccd.df
wccd.df <- rjson::fromJSON(crs.json)$SFObjects$wc_credit_decisions
wccd.df
wccd.df <- rjson::fromJSON(crs.json)$SFObjects$wc_credit_decisions
cr.df <- rjson::fromJSON(crs.json)$SFObjects$credit_reviews
opp.df <- rjson::fromJSON(crs.json)$SFObjects$opportunities
crbs.df <- rjson::fromJSON(crs.json)$SFObjects$bank_summaries
crml.df <- rjson::fromJSON(crs.json)$SFObjects$ledgers
crmr.df <- rjson::fromJSON(crs.json)$SFObjects$monthly_reviews
crmr.df
crml.df
crmr.df <- jsonlite::fromJSON(crs.json)$SFObjects$monthly_reviews
crml.df <- jsonlite::fromJSON(crs.json)$SFObjects$ledgers
crml.df$Avg_Daily_Balance__c
crs.json <- readChar(fileName, file.info(fileName)$size)
rjson::fromJSON(crs.json)
jsonlite::toJSON(rjson::fromJSON(crs.json))
crs.json <- jsonlite::toJSON(rjson::fromJSON(crs.json))
crs.json <- jsonlite::toJSON(rjson::fromJSON(rawJSON))
crml.df <- rjson::fromJSON(crs.json)$SFObjects$ledgers
crml.df
crml.df
crs.json <- jsonlite::toJSON(rjson::fromJSON(rawJSON))
crml.df <- jsonlite::fromJSON(crs.json)$SFObjects$ledgers
crml.df$Avg_Daily_Balance__c
rawJSON <- readChar(fileName, file.info(fileName)$size)
crs.json <- jsonlite::toJSON(rjson::fromJSON(rawJSON))
crs.json
wccd.df <- jsonlite::fromJSON(crs.json)$SFObjects$wc_credit_decisions
cr.df <- jsonlite::fromJSON(crs.json)$SFObjects$credit_reviews
opp.df <- jsonlite::fromJSON(crs.json)$SFObjects$opportunities
crbs.df <- jsonlite::fromJSON(crs.json)$SFObjects$bank_summaries
crml.df <- jsonlite::fromJSON(crs.json)$SFObjects$ledgers
crmr.df <- jsonlite::fromJSON(crs.json)$SFObjects$monthly_reviews
crml.df$Avg_Daily_Balance__c
crml.df$Avg_Daily_Balance__c
crs.json <- rjson::toJSON(rjson::fromJSON(rawJSON))
wccd.df <- jsonlite::fromJSON(crs.json)$SFObjects$wc_credit_decisions
cr.df <- jsonlite::fromJSON(crs.json)$SFObjects$credit_reviews
opp.df <- jsonlite::fromJSON(crs.json)$SFObjects$opportunities
crbs.df <- jsonlite::fromJSON(crs.json)$SFObjects$bank_summaries
crml.df <- jsonlite::fromJSON(crs.json)$SFObjects$ledgers
crmr.df <- jsonlite::fromJSON(crs.json)$SFObjects$monthly_reviews
crml.df$Avg_Daily_Balance__c
source('~/.active-rstudio-document', echo=TRUE)
